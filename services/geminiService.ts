// FIX: Removed `Type` from import as `responseSchema` is no longer used.
import { GoogleGenAI, Modality } from "@google/genai";
import type { GenerateContentResponse } from "@google/genai";
import type { GeminiResponse, OcrResult } from '../types';

if (!process.env.API_KEY || process.env.API_KEY === 'PLACEHOLDER_API_KEY') {
  throw new Error("GEMINI_API_KEY not configured. Please set a valid Gemini API key in .env.local");
}

const ai = new GoogleGenAI({ apiKey: process.env.API_KEY });

// FIX: Removed unused ocrSchema as `responseSchema` is not supported for `gemini-2.5-flash-image-preview` model.

export async function restoreImageAndExtractText(base64ImageData: string, mimeType: string): Promise<GeminiResponse> {
  const model = 'gemini-2.5-flash-image-preview';

  try {
    const response: GenerateContentResponse = await ai.models.generateContent({
      model: model,
      contents: {
        parts: [
          {
            inlineData: {
              data: base64ImageData,
              mimeType: mimeType,
            },
          },
          {
            text: `
              1. First, perform super-resolution and deblurring on this entire image to make it high-definition and clear. Make all text in the image sharper and more readable.
              2. Second, after enhancing, perform OCR on the image. Identify all text blocks, their bounding box positions (relative to image dimensions, from 0 to 1), and a confidence score.
              3. Return two things: the fully enhanced HD image, and a JSON object containing the OCR results. The JSON must be valid and parsable.
            `,
          },
        ],
      },
      // FIX: Removed `responseMimeType` and `responseSchema` as they are not supported for the `gemini-2.5-flash-image-preview` model according to guidelines.
      config: {
        responseModalities: [Modality.IMAGE, Modality.TEXT],
      },
    });

    let restoredImageBase64: string | null = null;
    let ocrResult: OcrResult | null = null;
    let textForOcr = '';

    if (!response.candidates || response.candidates.length === 0) {
      throw new Error("No content generated by the model.");
    }

    // FIX: Process multi-modal response by iterating through all parts to find image and text.
    // This is more robust than the previous implementation that used `response.text`.
    for (const part of response.candidates[0].content.parts) {
      if (part.inlineData) {
        restoredImageBase64 = part.inlineData.data;
      } else if (part.text) {
        textForOcr += part.text;
      }
    }
    
    // FIX: Made JSON parsing more lenient. If parsing fails, it no longer throws an error,
    // allowing the app to proceed with the restored image if it's available.
    if (textForOcr) {
      try {
          const cleanedText = textForOcr.trim().replace(/^```json\s*|```\s*$/g, '');
          const parsedJson = JSON.parse(cleanedText);
          if (parsedJson && Array.isArray(parsedJson.text_blocks)) {
               ocrResult = parsedJson as OcrResult;
          } else {
               console.warn("Parsed JSON does not match expected OCR schema", parsedJson);
          }
      } catch (e) {
          console.error("Failed to parse OCR JSON response:", textForOcr, e);
          // Do not throw an error here. Let it fall through so the restored image can still be shown.
      }
    }

    // If model fails to return an image, fall back to the original to prevent app from crashing.
    if (!restoredImageBase64) {
        console.warn("Model did not return an enhanced image. Falling back to original.");
        restoredImageBase64 = base64ImageData;
    }
    
    // If model fails to return OCR data, create an empty result to prevent app from crashing.
    if (!ocrResult) {
        console.warn("Model did not return valid OCR results.");
        ocrResult = { text_blocks: [] };
    }


    return { restoredImageBase64, ocrResult };

  } catch (error) {
    console.error('Error calling Gemini API:', error);
    if (error instanceof Error) {
        throw new Error(`Failed to communicate with the AI model: ${error.message}`);
    }
    throw new Error('An unknown error occurred while communicating with the AI model.');
  }
}

export async function repairImageRegion(base64ImageData: string, mimeType: string): Promise<string> {
  const model = 'gemini-2.5-flash-image-preview';

  try {
    const response: GenerateContentResponse = await ai.models.generateContent({
      model: model,
      contents: {
        parts: [
          {
            inlineData: {
              data: base64ImageData,
              mimeType: mimeType,
            },
          },
          {
            text: "The text in this image is blurry or hard to read. Make the text clearer and more readable. Return only the edited image, with no other text or explanation.",
          },
        ],
      },
      // FIX: Per guidelines for image editing model, both IMAGE and TEXT modalities must be included.
      config: {
        responseModalities: [Modality.IMAGE, Modality.TEXT],
      },
    });
    
    let repairedImageBase64: string | null = null;
    if (response.candidates && response.candidates.length > 0) {
        for (const part of response.candidates[0].content.parts) {
            if (part.inlineData) {
                repairedImageBase64 = part.inlineData.data;
                break; // Found the image, no need to look further
            }
        }
    }

    if (repairedImageBase64) {
      return repairedImageBase64;
    } else {
      console.warn("Image region repair did not return an image. Using original patch.");
      return base64ImageData; // Return original if repair fails
    }

  } catch (error) {
    console.error('Error calling Gemini API for image repair:', error);
    // Return original image data as a fallback to prevent total failure
    return base64ImageData;
  }
}